{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import dlib\n",
    "\n",
    "file_name = \"encodings/database.npz\"\n",
    "changed = False\n",
    "\n",
    "face_recognition_model = \"models/dlib_face_recognition_resnet_model_v1.dat\"\n",
    "face_encoder = None\n",
    "\n",
    "face_detector = None\n",
    "\n",
    "predictor_model = \"models/shape_predictor_5_face_landmarks.dat\"\n",
    "pose_predictor = None\n",
    "\n",
    "try:\n",
    "    known_face_encodings, known_face_labels = np.load(file_name).values()\n",
    "except IOError:\n",
    "\tknown_face_encodings, known_face_labels = np.array([]), np.array([], \"str\")\n",
    "\tchanged = True\n",
    "# print(\"loaded...\")\n",
    "\n",
    "def save_data():\n",
    "\tnp.savez(file_name, known_face_encodings, known_face_labels)\n",
    "\n",
    "def get_face_encoder():\n",
    "\tglobal face_encoder\n",
    "\tif face_encoder is None:\n",
    "\t\tface_encoder = dlib.face_recognition_model_v1(face_recognition_model)\n",
    "\treturn face_encoder\n",
    "\n",
    "def get_face_detector():\n",
    "\tglobal face_detector\n",
    "\n",
    "\tif face_detector is None:\n",
    "\t\t# can use cnn detector also default is hog(fast)\n",
    "\t\tface_detector = dlib.get_frontal_face_detector()\n",
    "\treturn face_detector\n",
    "\n",
    "def get_pose_predictor():\n",
    "\tglobal pose_predictor\n",
    "\tif pose_predictor is None:\n",
    "\t\tpose_predictor = dlib.shape_predictor(predictor_model)\n",
    "\treturn pose_predictor\n",
    "\n",
    "def load_image_file(file):\n",
    "\tim = Image.open(file)\n",
    "\tim = im.convert(\"RGB\")\n",
    "\treturn np.array(im)\n",
    "\n",
    "def css_to_rect(css):\n",
    "    return dlib.rectangle(css[3], css[0], css[1], css[2])\n",
    "\n",
    "def rect_to_css(rect):\n",
    "    return rect.top(), rect.right(), rect.bottom(), rect.left()\n",
    "\n",
    "def trim_css_to_bounds(css, image_shape):\n",
    "    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\n",
    "\n",
    "\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "\t# initialize the list of (x, y)-coordinates\n",
    "\tcoords = np.zeros((shape.num_parts, 2), dtype=dtype)\n",
    "\n",
    "\t# loop over all facial landmarks and convert them\n",
    "\t# to a 2-tuple of (x, y)-coordinates\n",
    "\tfor i in range(0, shape.num_parts):\n",
    "\t\tcoords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\n",
    "\t# return the list of (x, y)-coordinates\n",
    "\treturn coords\n",
    "\n",
    "def face_distance(face_encodings, face_to_compare):\n",
    "    if len(face_encodings) == 0:\n",
    "        return np.empty((0))\n",
    "\n",
    "    return np.linalg.norm(face_encodings - face_to_compare, axis=1)\n",
    "\n",
    "def _raw_face_landmarks(face_image, face_locations=None):\n",
    "\tif face_locations is None:\n",
    "\t\tface_locations = get_face_detector()(face_image, 1)\n",
    "\telse:\n",
    "\t\tface_locations = [css_to_rect(face_location) for face_location in face_locations]\n",
    "\n",
    "\treturn [get_pose_predictor()(face_image, face_location) for face_location in face_locations]\n",
    "\n",
    "\n",
    "def get_face_encodings(face_image, known_face_locations=None, num_jitters=1):\n",
    "\traw_landmarks = _raw_face_landmarks(face_image, known_face_locations)\n",
    "\treturn [np.array(get_face_encoder().compute_face_descriptor(\n",
    "\t\tface_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]\n",
    "\n",
    "def get_face_locations(img):\n",
    "\treturn [trim_css_to_bounds(rect_to_css(face), img.shape) for face in get_face_detector()(img, 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class video_capture:\n",
    "    \"\"\" Class to connect to remote camera \"\"\"\n",
    "    url = \"http://192.168.137.155:8080/shot.jpg\"\n",
    "    @staticmethod\n",
    "    def read():\n",
    "        imgResp = request.urlopen(__class__.url)\n",
    "        imgNp = np.array(bytearray(imgResp.read()), dtype=np.uint8)\n",
    "        img = cv2.imdecode(imgNp, -1)\n",
    "        # img = cv2.resize(img, (640, 480)) # use this if size recieve is very large\n",
    "        return None, img\n",
    "    @staticmethod\n",
    "    def release():\n",
    "        pass\n",
    "\n",
    "# video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    _, frame = video_capture.read()\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        times = 0.8\n",
    "        # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=times, fy=times)\n",
    "        # small_frame = frame.copy()\n",
    "\n",
    "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "        \n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = get_face_locations(rgb_small_frame)\n",
    "        face_encodings = get_face_encodings(rgb_small_frame, face_locations)\n",
    "        # print(\"Face detected... \", len(face_encodings))\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # Or instead, use the known face with the smallest distance to the new face\n",
    "            face_distances = face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            # if matches[best_match_index]:\n",
    "            if face_distances[best_match_index] < 0.5:\n",
    "                name = known_face_labels[best_match_index]\n",
    "            else:\n",
    "                name = \"Unknown\"\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        if times != 1:\n",
    "            top = int(top * (1/times))\n",
    "            right = int(right * (1/times))\n",
    "            bottom = int(bottom * (1/times))\n",
    "            left = int(left * (1/times))\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (255, 0, 0), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 720, 3) (384, 576, 3)\n"
     ]
    }
   ],
   "source": [
    "print(frame.shape, small_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.82601318e-01,  1.43306494e-01,  4.75299805e-02,  6.98941574e-03,\n",
       "        -3.61036733e-02, -4.37219553e-02, -2.94948369e-03, -4.28521819e-02,\n",
       "         1.40828177e-01, -3.18889953e-02,  2.86602706e-01, -6.66234493e-02,\n",
       "        -8.79895538e-02, -9.96640772e-02,  2.28629336e-02,  1.14016123e-01,\n",
       "        -1.46562397e-01, -1.55406907e-01, -4.16643992e-02, -6.29545078e-02,\n",
       "         6.94382042e-02, -5.84198870e-02, -2.96676420e-02,  9.32637155e-02,\n",
       "        -1.96350127e-01, -3.24195772e-01, -8.74499679e-02, -1.22879334e-01,\n",
       "         1.06417581e-01, -8.86754319e-02, -4.93622869e-02,  4.78196107e-02,\n",
       "        -1.98641151e-01, -4.06240821e-02, -3.49454880e-02,  8.46922323e-02,\n",
       "         6.89230207e-03,  4.84503806e-03,  1.10867813e-01,  2.21803412e-02,\n",
       "        -1.52630195e-01,  1.90328658e-02,  6.37322813e-02,  2.93712169e-01,\n",
       "         2.29291752e-01,  9.78410244e-02,  5.81969991e-02, -5.88554237e-03,\n",
       "        -8.65512341e-03, -1.72328621e-01,  6.87415451e-02,  1.09770179e-01,\n",
       "         1.31329671e-01,  7.59198144e-02,  6.08095080e-02, -1.89250439e-01,\n",
       "        -3.46088856e-02,  3.69404890e-02, -1.44265130e-01,  6.87874779e-02,\n",
       "        -1.75467506e-03,  2.87129804e-02, -9.72211547e-03, -2.97681578e-02,\n",
       "         2.64867663e-01,  1.35681599e-01, -8.76052827e-02, -1.17199332e-01,\n",
       "         1.71034530e-01, -1.58743218e-01, -5.30659035e-02,  9.95747596e-02,\n",
       "        -9.31533799e-02, -1.80349112e-01, -2.61593312e-01,  9.37919319e-02,\n",
       "         4.35918778e-01,  1.49445623e-01, -1.77969068e-01,  5.79544604e-02,\n",
       "        -6.60911202e-02, -6.12262934e-02,  5.37355244e-02,  5.83928451e-02,\n",
       "        -1.18247837e-01,  1.86274312e-02, -1.29346848e-01,  2.43921578e-03,\n",
       "         2.03778058e-01,  5.43066487e-02, -2.23202351e-02,  1.51594743e-01,\n",
       "        -1.51609778e-02, -1.90262198e-02,  1.69543885e-02,  1.06806085e-02,\n",
       "        -7.42672831e-02,  3.47897820e-02, -8.38765055e-02, -9.68847722e-02,\n",
       "        -1.58932637e-02, -6.94734901e-02, -5.73280752e-02,  9.22919959e-02,\n",
       "        -2.02758506e-01,  1.34981528e-01,  3.28383967e-02, -1.81555748e-04,\n",
       "        -9.27305073e-02,  1.15302935e-01, -7.74160028e-02,  2.50469893e-04,\n",
       "         1.05460882e-01, -2.66040772e-01,  1.50409818e-01,  1.95197746e-01,\n",
       "         5.05169630e-02,  1.49433121e-01,  1.05144270e-01,  7.04733059e-02,\n",
       "         3.71330939e-02,  4.87706438e-03, -1.16277926e-01, -2.66652778e-02,\n",
       "         1.43468007e-02, -9.14510489e-02,  6.93974569e-02,  8.10240358e-02])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_face_encodings(rgb_small_frame, face_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "004e3296fb33a75f912db67dfbc804ddcf50611225758f817e2dd7ebe1314606"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
